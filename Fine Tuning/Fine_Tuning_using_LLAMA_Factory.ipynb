{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Fine Tuning LLMs using LLAMA-FACTORY\n",
        "## Dataset Used\n",
        "**Natural Language to Docker Command Dataset**\n",
        "This dataset is designed to translate natural language instructions into Docker commands. It contains mappings of textual phrases to corresponding Docker commands, aiding in the development of models capable of understanding and translating user requests into executable Docker instructions.\n",
        "\n",
        "Dataset Format\n",
        "Each entry in the dataset consists of a JSON object with the following keys:\n",
        "* input: The natural language phrase.\n",
        "* instruction: A static field indicating the task to translate the phrase into a Docker command.\n",
        "* output: The corresponding Docker command.\n",
        "\n",
        "**1. Model Download and Initialization:**\n",
        "\n",
        "- Downloading shards: Indicates the process of downloading the model's weight shards.\n",
        "- Instantiating MistralForCausalLM model: The model is being instantiated with default data type torch.float16.\n",
        "- Loading checkpoint shards: The model’s checkpoints are being loaded.\n",
        "- All model checkpoint weights were used: Confirms that all weights from the checkpoint were successfully loaded into the model.\n",
        "\n",
        "**2. Model Configuration:**\n",
        "\n",
        "- Generate config GenerationConfig: A configuration object is created, specifying the beginning-of-sequence token (bos_token_id) and end-of-sequence token (eos_token_id).\n",
        "\n",
        "**3. Gradient Checkpointing and Fine-tuning:**\n",
        "\n",
        "- Gradient checkpointing enabled: This technique is used to reduce memory usage during training.\n",
        "- Using torch SDPA for faster training and inference: Indicates the use of a specific method for efficient training and inference.\n",
        "- Upcasting trainable params to float32: Trainable parameters are being upcasted to float32 for better precision during training.\n",
        "- Fine-tuning method: LoRA: The model is fine-tuned using Low-Rank Adaptation (LoRA), a technique to reduce the number of trainable parameters.\n",
        "\n",
        "**4. Training Details:**\n",
        "\n",
        "- Num examples = 2,415: The number of training examples.\n",
        "- Num Epochs = 1: The model is trained for one epoch.\n",
        "- Batch size and optimization steps: Details about batch size and the number of optimization steps.\n",
        "- Logging training progress: Loss, learning rate, gradient norm, and epoch progress are logged at regular intervals.\n",
        "\n",
        "**5. Training Metrics:**\n",
        "\n",
        "- Train loss and runtime metrics: At the end of training, the overall training loss and runtime metrics are reported.\n",
        "- Saving model checkpoint: The trained model checkpoint is saved to a specified directory.\n",
        "\n",
        "**6. Loading and Quantization:**\n",
        "\n",
        "- Loading tokenizer and configuration files: These files are essential for the model to process inputs correctly.\n",
        "- Quantizing model to 4 bit: Indicates that the model is being quantized to reduce its size and improve inference speed.\n",
        "\n",
        "**7. Using the Model for Inference:**\n",
        "\n",
        "- Loading weights and initializing the model: The model weights are loaded, and the model is initialized for inference.\n"
      ],
      "metadata": {
        "id": "0hM9p83YdgJX"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iaO9CDuwdfyn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yTErp8aRV8-y",
        "outputId": "a7393940-bdea-41fc-d64a-e733c0fb3d21"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'LLaMA-Factory'...\n",
            "remote: Enumerating objects: 259, done.\u001b[K\n",
            "remote: Counting objects: 100% (259/259), done.\u001b[K\n",
            "remote: Compressing objects: 100% (217/217), done.\u001b[K\n",
            "remote: Total 259 (delta 46), reused 150 (delta 31), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (259/259), 7.77 MiB | 15.07 MiB/s, done.\n",
            "Resolving deltas: 100% (46/46), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd '/content/LLaMA-Factory'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W1Oe942kWHe2",
        "outputId": "5a58f2bb-71c7-4339-b7b2-f29c53c92c80"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/LLaMA-Factory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -e .[torch,metrics]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QacBWzxvWJzu",
        "outputId": "9b2961b6-5f58-475f-d07c-4a7e98dd54af"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Obtaining file:///content/LLaMA-Factory\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: transformers>=4.37.2 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.7.2.dev0) (4.41.1)\n",
            "Collecting datasets>=2.14.3 (from llamafactory==0.7.2.dev0)\n",
            "  Downloading datasets-2.19.1-py3-none-any.whl (542 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting accelerate>=0.27.2 (from llamafactory==0.7.2.dev0)\n",
            "  Downloading accelerate-0.30.1-py3-none-any.whl (302 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.6/302.6 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting peft>=0.10.0 (from llamafactory==0.7.2.dev0)\n",
            "  Downloading peft-0.11.1-py3-none-any.whl (251 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.6/251.6 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting trl>=0.8.1 (from llamafactory==0.7.2.dev0)\n",
            "  Downloading trl-0.8.6-py3-none-any.whl (245 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.2/245.2 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gradio>=4.0.0 (from llamafactory==0.7.2.dev0)\n",
            "  Downloading gradio-4.32.1-py3-none-any.whl (12.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m46.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.7.2.dev0) (1.11.4)\n",
            "Collecting einops (from llamafactory==0.7.2.dev0)\n",
            "  Downloading einops-0.8.0-py3-none-any.whl (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.7.2.dev0) (0.1.99)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.7.2.dev0) (3.20.3)\n",
            "Collecting uvicorn (from llamafactory==0.7.2.dev0)\n",
            "  Downloading uvicorn-0.30.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.7.2.dev0) (2.7.1)\n",
            "Collecting fastapi (from llamafactory==0.7.2.dev0)\n",
            "  Downloading fastapi-0.111.0-py3-none-any.whl (91 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sse-starlette (from llamafactory==0.7.2.dev0)\n",
            "  Downloading sse_starlette-2.1.0-py3-none-any.whl (9.2 kB)\n",
            "Requirement already satisfied: matplotlib>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.7.2.dev0) (3.7.1)\n",
            "Collecting fire (from llamafactory==0.7.2.dev0)\n",
            "  Downloading fire-0.6.0.tar.gz (88 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.4/88.4 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.7.2.dev0) (24.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.7.2.dev0) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.13.1 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.7.2.dev0) (2.3.0+cu121)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.7.2.dev0) (3.8.1)\n",
            "Requirement already satisfied: jieba in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.7.2.dev0) (0.42.1)\n",
            "Collecting rouge-chinese (from llamafactory==0.7.2.dev0)\n",
            "  Downloading rouge_chinese-1.0.3-py3-none-any.whl (21 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.27.2->llamafactory==0.7.2.dev0) (1.25.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.27.2->llamafactory==0.7.2.dev0) (5.9.5)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.27.2->llamafactory==0.7.2.dev0) (0.23.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.27.2->llamafactory==0.7.2.dev0) (0.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.14.3->llamafactory==0.7.2.dev0) (3.14.0)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.14.3->llamafactory==0.7.2.dev0) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets>=2.14.3->llamafactory==0.7.2.dev0) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets>=2.14.3->llamafactory==0.7.2.dev0)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets>=2.14.3->llamafactory==0.7.2.dev0) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.14.3->llamafactory==0.7.2.dev0) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.14.3->llamafactory==0.7.2.dev0) (4.66.4)\n",
            "Collecting xxhash (from datasets>=2.14.3->llamafactory==0.7.2.dev0)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets>=2.14.3->llamafactory==0.7.2.dev0)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.14.3->llamafactory==0.7.2.dev0) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.14.3->llamafactory==0.7.2.dev0) (3.9.5)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio>=4.0.0->llamafactory==0.7.2.dev0)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.7.2.dev0) (4.2.2)\n",
            "Collecting ffmpy (from gradio>=4.0.0->llamafactory==0.7.2.dev0)\n",
            "  Downloading ffmpy-0.3.2.tar.gz (5.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gradio-client==0.17.0 (from gradio>=4.0.0->llamafactory==0.7.2.dev0)\n",
            "  Downloading gradio_client-0.17.0-py3-none-any.whl (316 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.3/316.3 kB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpx>=0.24.1 (from gradio>=4.0.0->llamafactory==0.7.2.dev0)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.7.2.dev0) (6.4.0)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.7.2.dev0) (3.1.4)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.7.2.dev0) (2.1.5)\n",
            "Collecting orjson~=3.0 (from gradio>=4.0.0->llamafactory==0.7.2.dev0)\n",
            "  Downloading orjson-3.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.7.2.dev0) (9.4.0)\n",
            "Collecting pydub (from gradio>=4.0.0->llamafactory==0.7.2.dev0)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Collecting python-multipart>=0.0.9 (from gradio>=4.0.0->llamafactory==0.7.2.dev0)\n",
            "  Downloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\n",
            "Collecting ruff>=0.2.2 (from gradio>=4.0.0->llamafactory==0.7.2.dev0)\n",
            "  Downloading ruff-0.4.7-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m90.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting semantic-version~=2.0 (from gradio>=4.0.0->llamafactory==0.7.2.dev0)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting tomlkit==0.12.0 (from gradio>=4.0.0->llamafactory==0.7.2.dev0)\n",
            "  Downloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
            "Collecting typer<1.0,>=0.12 (from gradio>=4.0.0->llamafactory==0.7.2.dev0)\n",
            "  Downloading typer-0.12.3-py3-none-any.whl (47 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.2/47.2 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.7.2.dev0) (4.11.0)\n",
            "Requirement already satisfied: urllib3~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.7.2.dev0) (2.0.7)\n",
            "Collecting websockets<12.0,>=10.0 (from gradio-client==0.17.0->gradio>=4.0.0->llamafactory==0.7.2.dev0)\n",
            "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.7.2.dev0) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.7.2.dev0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.7.2.dev0) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.7.2.dev0) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.7.2.dev0) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.7.2.dev0) (2.8.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic->llamafactory==0.7.2.dev0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic->llamafactory==0.7.2.dev0) (2.18.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.7.2.dev0) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.7.2.dev0) (3.3)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.13.1->llamafactory==0.7.2.dev0)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.13.1->llamafactory==0.7.2.dev0)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.13.1->llamafactory==0.7.2.dev0)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.13.1->llamafactory==0.7.2.dev0)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.13.1->llamafactory==0.7.2.dev0)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.13.1->llamafactory==0.7.2.dev0)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.13.1->llamafactory==0.7.2.dev0)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.13.1->llamafactory==0.7.2.dev0)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.13.1->llamafactory==0.7.2.dev0)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.13.1->llamafactory==0.7.2.dev0)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.13.1->llamafactory==0.7.2.dev0)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.7.2.dev0) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13.1->llamafactory==0.7.2.dev0)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m68.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.37.2->llamafactory==0.7.2.dev0) (2024.5.15)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.37.2->llamafactory==0.7.2.dev0) (0.19.1)\n",
            "Collecting tyro>=0.5.11 (from trl>=0.8.1->llamafactory==0.7.2.dev0)\n",
            "  Downloading tyro-0.8.4-py3-none-any.whl (102 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.4/102.4 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn->llamafactory==0.7.2.dev0) (8.1.7)\n",
            "Collecting h11>=0.8 (from uvicorn->llamafactory==0.7.2.dev0)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting starlette<0.38.0,>=0.37.2 (from fastapi->llamafactory==0.7.2.dev0)\n",
            "  Downloading starlette-0.37.2-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fastapi-cli>=0.0.2 (from fastapi->llamafactory==0.7.2.dev0)\n",
            "  Downloading fastapi_cli-0.0.4-py3-none-any.whl (9.5 kB)\n",
            "Collecting ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 (from fastapi->llamafactory==0.7.2.dev0)\n",
            "  Downloading ujson-5.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting email_validator>=2.0.0 (from fastapi->llamafactory==0.7.2.dev0)\n",
            "  Downloading email_validator-2.1.1-py3-none-any.whl (30 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from fire->llamafactory==0.7.2.dev0) (1.16.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire->llamafactory==0.7.2.dev0) (2.4.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->llamafactory==0.7.2.dev0) (1.4.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from sse-starlette->llamafactory==0.7.2.dev0) (3.7.1)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio>=4.0.0->llamafactory==0.7.2.dev0) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio>=4.0.0->llamafactory==0.7.2.dev0) (4.19.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio>=4.0.0->llamafactory==0.7.2.dev0) (0.12.1)\n",
            "Collecting dnspython>=2.0.0 (from email_validator>=2.0.0->fastapi->llamafactory==0.7.2.dev0)\n",
            "  Downloading dnspython-2.6.1-py3-none-any.whl (307 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: idna>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from email_validator>=2.0.0->fastapi->llamafactory==0.7.2.dev0) (3.7)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.14.3->llamafactory==0.7.2.dev0) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.14.3->llamafactory==0.7.2.dev0) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.14.3->llamafactory==0.7.2.dev0) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.14.3->llamafactory==0.7.2.dev0) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.14.3->llamafactory==0.7.2.dev0) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.14.3->llamafactory==0.7.2.dev0) (4.0.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio>=4.0.0->llamafactory==0.7.2.dev0) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx>=0.24.1->gradio>=4.0.0->llamafactory==0.7.2.dev0)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio>=4.0.0->llamafactory==0.7.2.dev0) (1.3.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.14.3->llamafactory==0.7.2.dev0) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.14.3->llamafactory==0.7.2.dev0) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.14.3->llamafactory==0.7.2.dev0) (3.3.2)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->sse-starlette->llamafactory==0.7.2.dev0) (1.2.1)\n",
            "Collecting shellingham>=1.3.0 (from typer<1.0,>=0.12->gradio>=4.0.0->llamafactory==0.7.2.dev0)\n",
            "  Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio>=4.0.0->llamafactory==0.7.2.dev0) (13.7.1)\n",
            "Requirement already satisfied: docstring-parser>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl>=0.8.1->llamafactory==0.7.2.dev0) (0.16)\n",
            "Collecting shtab>=1.5.6 (from tyro>=0.5.11->trl>=0.8.1->llamafactory==0.7.2.dev0)\n",
            "  Downloading shtab-1.7.1-py3-none-any.whl (14 kB)\n",
            "Collecting httptools>=0.5.0 (from uvicorn->llamafactory==0.7.2.dev0)\n",
            "  Downloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (341 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-dotenv>=0.13 (from uvicorn->llamafactory==0.7.2.dev0)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn->llamafactory==0.7.2.dev0)\n",
            "  Downloading uvloop-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m103.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting watchfiles>=0.13 (from uvicorn->llamafactory==0.7.2.dev0)\n",
            "  Downloading watchfiles-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m71.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.1->llamafactory==0.7.2.dev0) (1.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio>=4.0.0->llamafactory==0.7.2.dev0) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio>=4.0.0->llamafactory==0.7.2.dev0) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio>=4.0.0->llamafactory==0.7.2.dev0) (0.18.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio>=4.0.0->llamafactory==0.7.2.dev0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio>=4.0.0->llamafactory==0.7.2.dev0) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio>=4.0.0->llamafactory==0.7.2.dev0) (0.1.2)\n",
            "Building wheels for collected packages: fire, llamafactory, ffmpy\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.6.0-py2.py3-none-any.whl size=117029 sha256=963378b7883793d6e37e859ba3124aa136eb36c359db3780b27312b9634fe8fe\n",
            "  Stored in directory: /root/.cache/pip/wheels/d6/6d/5d/5b73fa0f46d01a793713f8859201361e9e581ced8c75e5c6a3\n",
            "  Building editable for llamafactory (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llamafactory: filename=llamafactory-0.7.2.dev0-0.editable-py3-none-any.whl size=18708 sha256=575b6684ebac29a6c97d2ab215ae4754d520197ea27e532c4718d227a82b4905\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-elk25wrx/wheels/de/aa/c5/27b5682c5592b7c0eecc3e208f176dedf6b11a61cf2a910b85\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpy: filename=ffmpy-0.3.2-py3-none-any.whl size=5584 sha256=1240dd8eaf3ee48eef230f793567ee058f18cea2f19c212cd317529086db58ee\n",
            "  Stored in directory: /root/.cache/pip/wheels/bd/65/9a/671fc6dcde07d4418df0c592f8df512b26d7a0029c2a23dd81\n",
            "Successfully built fire llamafactory ffmpy\n",
            "Installing collected packages: pydub, ffmpy, xxhash, websockets, uvloop, ujson, tomlkit, shtab, shellingham, semantic-version, ruff, rouge-chinese, python-multipart, python-dotenv, orjson, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, httptools, h11, fire, einops, dnspython, dill, aiofiles, watchfiles, uvicorn, starlette, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, httpcore, email_validator, tyro, typer, sse-starlette, nvidia-cusolver-cu12, httpx, gradio-client, fastapi-cli, datasets, fastapi, accelerate, trl, peft, gradio, llamafactory\n",
            "  Attempting uninstall: typer\n",
            "    Found existing installation: typer 0.9.4\n",
            "    Uninstalling typer-0.9.4:\n",
            "      Successfully uninstalled typer-0.9.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "spacy 3.7.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\n",
            "weasel 0.3.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed accelerate-0.30.1 aiofiles-23.2.1 datasets-2.19.1 dill-0.3.8 dnspython-2.6.1 einops-0.8.0 email_validator-2.1.1 fastapi-0.111.0 fastapi-cli-0.0.4 ffmpy-0.3.2 fire-0.6.0 gradio-4.32.1 gradio-client-0.17.0 h11-0.14.0 httpcore-1.0.5 httptools-0.6.1 httpx-0.27.0 llamafactory-0.7.2.dev0 multiprocess-0.70.16 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 orjson-3.10.3 peft-0.11.1 pydub-0.25.1 python-dotenv-1.0.1 python-multipart-0.0.9 rouge-chinese-1.0.3 ruff-0.4.7 semantic-version-2.10.0 shellingham-1.5.4 shtab-1.7.1 sse-starlette-2.1.0 starlette-0.37.2 tomlkit-0.12.0 trl-0.8.6 typer-0.12.3 tyro-0.8.4 ujson-5.10.0 uvicorn-0.30.0 uvloop-0.19.0 watchfiles-0.22.0 websockets-11.0.3 xxhash-3.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bitsandbytes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EyS3CCpcWLmd",
        "outputId": "c2795143-6347-4544-e559-8569f8024c7b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl (119.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.8/119.8 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.3.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.25.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->bitsandbytes) (12.5.40)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->bitsandbytes) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->bitsandbytes) (1.3.0)\n",
            "Installing collected packages: bitsandbytes\n",
            "Successfully installed bitsandbytes-0.43.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade huggingface_hub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cTcHpc4PWRkh",
        "outputId": "1a8d33ff-7acc-4d11-cba5-06a8bc61b776"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.23.1)\n",
            "Collecting huggingface_hub\n",
            "  Downloading huggingface_hub-0.23.2-py3-none-any.whl (401 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.7/401.7 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.14.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2023.6.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2024.2.2)\n",
            "Installing collected packages: huggingface_hub\n",
            "  Attempting uninstall: huggingface_hub\n",
            "    Found existing installation: huggingface-hub 0.23.1\n",
            "    Uninstalling huggingface-hub-0.23.1:\n",
            "      Successfully uninstalled huggingface-hub-0.23.1\n",
            "Successfully installed huggingface_hub-0.23.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gzKj_zOFWRiR",
        "outputId": "c59a5cfc-0ccc-4aee-97be-955f53e3e5f8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) Y\n",
            "Token is valid (permission: read).\n",
            "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
            "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
            "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
            "\n",
            "git config --global credential.helper store\n",
            "\n",
            "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
            "Token has not been saved to git credential helper.\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!CUDA_VISIBLE_DEVICES=0 GRADIO_SHARE=1 llamafactory-cli webui"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QIwLHrWxWRgD",
        "outputId": "920650a3-bc27-444a-be80-338fe1b40451"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-06-01 10:39:42.225883: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-06-01 10:39:42.225943: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-06-01 10:39:42.227448: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-06-01 10:39:42.235330: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-06-01 10:39:43.747749: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Running on local URL:  http://0.0.0.0:7860\n",
            "Running on public URL: https://bd22faea76d3f12293.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/queueing.py\", line 521, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/route_utils.py\", line 276, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1945, in process_api\n",
            "    result = await self.call_function(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1513, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/to_thread.py\", line 33, in run_sync\n",
            "    return await get_asynclib().run_sync_in_worker_thread(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 877, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 807, in run\n",
            "    result = context.run(func, *args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/utils.py\", line 831, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "  File \"/content/LLaMA-Factory/src/llamafactory/webui/utils.py\", line 183, in check_output_dir\n",
            "    if os.path.isdir(get_save_dir(model_name, finetuning_type, output_dir)):\n",
            "  File \"/content/LLaMA-Factory/src/llamafactory/webui/common.py\", line 44, in get_save_dir\n",
            "    return os.path.join(DEFAULT_SAVE_DIR, *paths)\n",
            "  File \"/content/LLaMA-Factory/src/llamafactory/webui/common.py\", line 43, in <genexpr>\n",
            "    paths = (path.replace(os.path.sep, \"\").replace(\" \", \"\").strip() for path in paths)\n",
            "AttributeError: 'NoneType' object has no attribute 'replace'\n",
            "2024-06-01 10:42:49.040234: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-06-01 10:42:49.040291: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-06-01 10:42:49.041606: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-06-01 10:42:50.168644: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "06/01/2024 10:42:56 - WARNING - llamafactory.hparams.parser - We recommend enable `upcast_layernorm` in quantized training.\n",
            "06/01/2024 10:42:56 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.float16\n",
            "tokenizer_config.json: 100% 1.47k/1.47k [00:00<00:00, 8.16MB/s]\n",
            "tokenizer.model: 100% 493k/493k [00:00<00:00, 13.7MB/s]\n",
            "tokenizer.json: 100% 1.80M/1.80M [00:00<00:00, 5.78MB/s]\n",
            "special_tokens_map.json: 100% 72.0/72.0 [00:00<00:00, 493kB/s]\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-06-01 10:42:57,684 >> loading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.1/snapshots/73068f3702d050a2fd5aa2ca1e612e5036429398/tokenizer.model\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-06-01 10:42:57,684 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.1/snapshots/73068f3702d050a2fd5aa2ca1e612e5036429398/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-06-01 10:42:57,684 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-06-01 10:42:57,685 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.1/snapshots/73068f3702d050a2fd5aa2ca1e612e5036429398/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-06-01 10:42:57,685 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.1/snapshots/73068f3702d050a2fd5aa2ca1e612e5036429398/tokenizer_config.json\n",
            "06/01/2024 10:42:57 - INFO - llamafactory.data.template - Add pad token: </s>\n",
            "06/01/2024 10:42:57 - INFO - llamafactory.data.loader - Loading dataset MattCoddity/dockerNLcommands...\n",
            "Downloading readme: 100% 1.46k/1.46k [00:00<00:00, 9.72MB/s]\n",
            "Downloading data: 100% 543k/543k [00:00<00:00, 1.54MB/s]\n",
            "Generating train split: 100% 2415/2415 [00:00<00:00, 102668.20 examples/s]\n",
            "/usr/local/lib/python3.10/dist-packages/multiprocess/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "Converting format of dataset (num_proc=16): 100% 2415/2415 [00:00<00:00, 3883.67 examples/s]\n",
            "Running tokenizer on dataset (num_proc=16): 100% 2415/2415 [00:02<00:00, 923.50 examples/s] \n",
            "input_ids:\n",
            "[1, 733, 16289, 28793, 17824, 456, 12271, 297, 281, 14295, 3445, 13, 8398, 528, 272, 6203, 1159, 345, 817, 25032, 28747, 21528, 2586, 733, 28748, 16289, 28793, 281, 14295, 6203, 387, 28722, 1159, 28746, 817, 25032, 28747, 21528, 2]\n",
            "inputs:\n",
            "<s> [INST] translate this sentence in docker command\n",
            "Show me the images before \"nginx:latest\". [/INST] docker images -f before=nginx:latest</s>\n",
            "label_ids:\n",
            "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 281, 14295, 6203, 387, 28722, 1159, 28746, 817, 25032, 28747, 21528, 2]\n",
            "labels:\n",
            "docker images -f before=nginx:latest</s>\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "config.json: 100% 571/571 [00:00<00:00, 3.84MB/s]\n",
            "[INFO|configuration_utils.py:733] 2024-06-01 10:43:06,430 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.1/snapshots/73068f3702d050a2fd5aa2ca1e612e5036429398/config.json\n",
            "[INFO|configuration_utils.py:796] 2024-06-01 10:43:06,433 >> Model config MistralConfig {\n",
            "  \"_name_or_path\": \"mistralai/Mistral-7B-Instruct-v0.1\",\n",
            "  \"architectures\": [\n",
            "    \"MistralForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"model_type\": \"mistral\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"sliding_window\": 4096,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.41.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "06/01/2024 10:43:06 - INFO - llamafactory.model.utils.quantization - Quantizing model to 4 bit.\n",
            "model.safetensors.index.json: 100% 25.1k/25.1k [00:00<00:00, 86.3MB/s]\n",
            "[INFO|modeling_utils.py:3474] 2024-06-01 10:43:06,931 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.1/snapshots/73068f3702d050a2fd5aa2ca1e612e5036429398/model.safetensors.index.json\n",
            "Downloading shards:   0% 0/2 [00:00<?, ?it/s]\n",
            "model-00001-of-00002.safetensors:   0% 0.00/9.94G [00:00<?, ?B/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   0% 21.0M/9.94G [00:00<00:49, 202MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   1% 52.4M/9.94G [00:00<00:44, 222MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   1% 83.9M/9.94G [00:00<00:43, 226MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   1% 115M/9.94G [00:00<00:43, 225MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:   1% 147M/9.94G [00:00<00:47, 205MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   2% 168M/9.94G [00:00<00:48, 201MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   2% 189M/9.94G [00:00<00:50, 193MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   2% 210M/9.94G [00:01<00:51, 187MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   2% 231M/9.94G [00:01<00:50, 191MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   3% 252M/9.94G [00:01<00:49, 195MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   3% 273M/9.94G [00:01<00:50, 190MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   3% 304M/9.94G [00:01<00:47, 201MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   3% 325M/9.94G [00:01<00:53, 179MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   3% 346M/9.94G [00:01<00:52, 182MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   4% 377M/9.94G [00:01<00:52, 181MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   4% 398M/9.94G [00:02<00:55, 171MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   4% 419M/9.94G [00:02<00:56, 169MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   4% 440M/9.94G [00:02<00:53, 177MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   5% 461M/9.94G [00:02<00:53, 177MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   5% 482M/9.94G [00:02<00:56, 167MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   5% 503M/9.94G [00:02<00:58, 161MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   5% 524M/9.94G [00:06<08:07, 19.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   6% 556M/9.94G [00:06<05:13, 29.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   6% 587M/9.94G [00:06<03:35, 43.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   6% 619M/9.94G [00:06<02:36, 59.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   6% 640M/9.94G [00:06<02:11, 70.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   7% 661M/9.94G [00:06<01:50, 84.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   7% 682M/9.94G [00:06<01:33, 99.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   7% 703M/9.94G [00:06<01:19, 116MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:   7% 734M/9.94G [00:07<01:05, 140MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   8% 765M/9.94G [00:07<00:57, 159MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   8% 786M/9.94G [00:07<00:54, 167MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   8% 807M/9.94G [00:07<00:52, 176MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   8% 839M/9.94G [00:07<00:47, 193MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   9% 870M/9.94G [00:07<00:44, 205MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   9% 902M/9.94G [00:07<00:42, 215MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   9% 933M/9.94G [00:08<00:42, 211MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  10% 965M/9.94G [00:08<00:43, 204MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  10% 996M/9.94G [00:08<00:43, 203MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  10% 1.02G/9.94G [00:08<00:43, 204MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  10% 1.04G/9.94G [00:08<00:44, 200MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  11% 1.06G/9.94G [00:09<02:51, 51.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  11% 1.08G/9.94G [00:09<02:16, 64.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  11% 1.10G/9.94G [00:10<01:50, 80.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  11% 1.12G/9.94G [00:10<01:31, 96.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  11% 1.14G/9.94G [00:10<01:17, 114MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  12% 1.16G/9.94G [00:10<01:08, 129MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  12% 1.18G/9.94G [00:10<01:01, 142MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  12% 1.21G/9.94G [00:10<00:56, 155MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  12% 1.23G/9.94G [00:10<00:52, 166MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  13% 1.26G/9.94G [00:10<00:48, 180MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  13% 1.28G/9.94G [00:10<00:46, 185MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  13% 1.30G/9.94G [00:11<00:46, 185MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  13% 1.32G/9.94G [00:11<00:46, 185MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  13% 1.34G/9.94G [00:11<00:45, 189MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  14% 1.36G/9.94G [00:11<00:45, 190MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  14% 1.38G/9.94G [00:13<03:56, 36.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  14% 1.41G/9.94G [00:13<02:59, 47.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  14% 1.44G/9.94G [00:13<02:03, 68.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  15% 1.47G/9.94G [00:13<01:32, 91.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  15% 1.50G/9.94G [00:13<01:14, 113MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  15% 1.52G/9.94G [00:13<01:08, 124MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  16% 1.54G/9.94G [00:13<01:03, 132MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  16% 1.56G/9.94G [00:13<00:58, 143MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  16% 1.58G/9.94G [00:15<02:53, 48.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  16% 1.60G/9.94G [00:15<02:22, 58.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  16% 1.63G/9.94G [00:15<01:55, 72.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  17% 1.66G/9.94G [00:15<01:26, 95.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  17% 1.69G/9.94G [00:15<01:12, 114MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  17% 1.71G/9.94G [00:15<01:07, 123MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  17% 1.73G/9.94G [00:16<01:02, 131MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  18% 1.75G/9.94G [00:16<00:59, 138MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  18% 1.77G/9.94G [00:16<00:55, 147MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  18% 1.79G/9.94G [00:16<00:52, 155MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  18% 1.81G/9.94G [00:16<00:49, 165MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  18% 1.84G/9.94G [00:16<00:46, 175MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  19% 1.86G/9.94G [00:16<00:44, 183MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  19% 1.88G/9.94G [00:16<00:45, 176MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  19% 1.90G/9.94G [00:16<00:46, 173MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  19% 1.92G/9.94G [00:17<00:46, 174MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  20% 1.94G/9.94G [00:17<00:45, 174MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  20% 1.96G/9.94G [00:17<00:45, 174MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  20% 1.98G/9.94G [00:17<01:27, 91.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  20% 2.00G/9.94G [00:20<06:04, 21.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  20% 2.02G/9.94G [00:20<04:26, 29.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  21% 2.04G/9.94G [00:20<03:18, 39.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  21% 2.07G/9.94G [00:20<02:30, 52.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  21% 2.09G/9.94G [00:20<01:57, 66.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  21% 2.11G/9.94G [00:21<01:34, 83.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  21% 2.13G/9.94G [00:21<01:17, 101MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  22% 2.15G/9.94G [00:21<01:05, 118MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  22% 2.18G/9.94G [00:21<00:54, 141MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  22% 2.21G/9.94G [00:21<00:47, 161MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  22% 2.23G/9.94G [00:21<00:45, 171MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  23% 2.25G/9.94G [00:21<00:42, 180MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  23% 2.29G/9.94G [00:21<00:40, 189MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  23% 2.31G/9.94G [00:21<00:39, 193MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  23% 2.33G/9.94G [00:22<00:39, 193MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  24% 2.35G/9.94G [00:22<00:38, 196MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  24% 2.37G/9.94G [00:22<00:39, 192MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  24% 2.40G/9.94G [00:22<00:37, 199MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  24% 2.43G/9.94G [00:22<00:35, 211MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  25% 2.46G/9.94G [00:22<00:34, 215MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  25% 2.50G/9.94G [00:22<00:33, 222MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  25% 2.53G/9.94G [00:23<00:34, 214MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  26% 2.56G/9.94G [00:23<00:36, 203MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  26% 2.58G/9.94G [00:23<00:36, 205MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  26% 2.60G/9.94G [00:23<00:35, 205MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  26% 2.63G/9.94G [00:23<00:34, 209MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  27% 2.66G/9.94G [00:24<01:54, 63.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  27% 2.68G/9.94G [00:24<01:38, 73.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  27% 2.71G/9.94G [00:24<01:22, 88.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  27% 2.73G/9.94G [00:25<01:09, 103MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  28% 2.75G/9.94G [00:25<01:01, 118MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  28% 2.78G/9.94G [00:25<00:49, 145MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  28% 2.81G/9.94G [00:25<00:42, 166MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  29% 2.84G/9.94G [00:25<00:38, 185MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  29% 2.87G/9.94G [00:25<00:37, 189MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  29% 2.90G/9.94G [00:25<00:38, 184MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  29% 2.93G/9.94G [00:26<00:37, 187MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  30% 2.95G/9.94G [00:26<00:36, 192MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  30% 2.98G/9.94G [00:26<00:34, 200MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  30% 3.01G/9.94G [00:26<00:33, 208MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  31% 3.04G/9.94G [00:26<00:32, 213MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  31% 3.07G/9.94G [00:26<00:31, 219MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  31% 3.10G/9.94G [00:26<00:31, 220MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  32% 3.14G/9.94G [00:27<00:32, 209MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  32% 3.17G/9.94G [00:27<00:32, 207MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  32% 3.20G/9.94G [00:27<00:32, 207MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  32% 3.22G/9.94G [00:27<00:32, 204MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  33% 3.24G/9.94G [00:28<01:15, 88.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  33% 3.26G/9.94G [00:30<04:28, 24.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  33% 3.29G/9.94G [00:30<03:02, 36.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  33% 3.32G/9.94G [00:30<02:10, 50.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  34% 3.36G/9.94G [00:31<01:37, 67.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  34% 3.38G/9.94G [00:31<01:22, 79.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  34% 3.40G/9.94G [00:31<01:11, 92.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  34% 3.42G/9.94G [00:31<01:01, 106MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  35% 3.44G/9.94G [00:31<00:53, 121MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  35% 3.47G/9.94G [00:31<01:00, 107MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  35% 3.49G/9.94G [00:32<00:53, 120MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  35% 3.51G/9.94G [00:32<00:48, 134MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  36% 3.54G/9.94G [00:32<00:40, 158MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  36% 3.58G/9.94G [00:32<00:37, 169MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  36% 3.60G/9.94G [00:32<00:35, 176MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  36% 3.62G/9.94G [00:32<00:37, 170MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  37% 3.64G/9.94G [00:32<00:39, 158MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  37% 3.66G/9.94G [00:32<00:37, 167MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  37% 3.68G/9.94G [00:36<05:54, 17.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  37% 3.71G/9.94G [00:36<03:48, 27.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  38% 3.74G/9.94G [00:37<02:36, 39.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  38% 3.77G/9.94G [00:37<01:52, 54.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  38% 3.80G/9.94G [00:37<01:32, 66.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  38% 3.82G/9.94G [00:37<01:16, 79.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  39% 3.84G/9.94G [00:37<01:04, 95.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  39% 3.86G/9.94G [00:37<00:54, 111MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  39% 3.88G/9.94G [00:37<00:47, 128MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  39% 3.90G/9.94G [00:37<00:42, 143MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  40% 3.93G/9.94G [00:37<00:36, 165MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  40% 3.95G/9.94G [00:38<00:34, 175MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  40% 3.98G/9.94G [00:38<00:31, 191MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  40% 4.02G/9.94G [00:38<00:30, 194MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  41% 4.05G/9.94G [00:38<00:28, 206MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  41% 4.08G/9.94G [00:38<00:28, 206MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  41% 4.11G/9.94G [00:38<00:29, 198MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  42% 4.13G/9.94G [00:38<00:30, 188MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  42% 4.15G/9.94G [00:39<00:30, 191MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  42% 4.17G/9.94G [00:39<01:13, 78.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  42% 4.19G/9.94G [00:40<01:34, 60.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  42% 4.22G/9.94G [00:40<01:17, 73.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  43% 4.24G/9.94G [00:40<01:06, 86.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  43% 4.26G/9.94G [00:40<00:55, 102MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  43% 4.28G/9.94G [00:40<00:49, 115MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  43% 4.30G/9.94G [00:40<00:45, 125MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  43% 4.32G/9.94G [00:41<00:41, 136MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  44% 4.34G/9.94G [00:41<00:39, 144MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  44% 4.36G/9.94G [00:41<00:37, 150MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  44% 4.39G/9.94G [00:41<00:32, 169MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  44% 4.41G/9.94G [00:41<00:31, 177MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  45% 4.45G/9.94G [00:41<00:29, 188MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  45% 4.47G/9.94G [00:41<00:28, 191MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  45% 4.49G/9.94G [00:41<00:32, 166MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  45% 4.51G/9.94G [00:42<00:32, 166MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  46% 4.53G/9.94G [00:42<00:31, 171MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  46% 4.55G/9.94G [00:42<00:33, 163MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  46% 4.58G/9.94G [00:42<00:43, 124MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  46% 4.60G/9.94G [00:42<00:40, 132MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  47% 4.62G/9.94G [00:42<00:36, 144MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  47% 4.65G/9.94G [00:43<00:34, 155MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  47% 4.67G/9.94G [00:43<00:31, 165MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  47% 4.70G/9.94G [00:43<00:29, 179MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  47% 4.72G/9.94G [00:43<00:29, 178MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  48% 4.74G/9.94G [00:43<00:31, 168MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  48% 4.76G/9.94G [00:43<00:31, 165MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  48% 4.78G/9.94G [00:43<00:31, 166MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  48% 4.80G/9.94G [00:43<00:31, 165MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  49% 4.82G/9.94G [00:47<04:01, 21.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  49% 4.85G/9.94G [00:47<02:35, 32.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  49% 4.89G/9.94G [00:47<01:47, 47.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  49% 4.92G/9.94G [00:47<01:18, 64.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  50% 4.94G/9.94G [00:47<01:06, 75.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  50% 4.97G/9.94G [00:47<00:51, 96.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  50% 5.00G/9.94G [00:47<00:41, 119MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  51% 5.03G/9.94G [00:48<00:34, 141MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  51% 5.06G/9.94G [00:48<00:31, 155MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  51% 5.09G/9.94G [00:48<00:30, 159MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  51% 5.11G/9.94G [00:48<00:29, 164MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  52% 5.13G/9.94G [00:48<00:28, 169MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  52% 5.15G/9.94G [00:48<00:27, 173MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  52% 5.18G/9.94G [00:48<00:25, 186MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  52% 5.20G/9.94G [00:48<00:24, 191MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  53% 5.22G/9.94G [00:48<00:24, 195MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  53% 5.25G/9.94G [00:49<00:23, 201MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  53% 5.28G/9.94G [00:49<00:22, 209MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  53% 5.32G/9.94G [00:49<00:21, 217MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  54% 5.35G/9.94G [00:49<00:20, 221MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  54% 5.38G/9.94G [00:49<00:20, 223MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  54% 5.41G/9.94G [00:49<00:22, 199MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  55% 5.43G/9.94G [00:49<00:22, 201MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  55% 5.46G/9.94G [00:50<00:21, 204MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  55% 5.48G/9.94G [00:50<00:22, 200MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  55% 5.52G/9.94G [00:50<00:22, 201MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  56% 5.54G/9.94G [00:50<00:22, 199MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  56% 5.56G/9.94G [00:50<00:22, 199MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  56% 5.58G/9.94G [00:50<00:22, 196MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  56% 5.60G/9.94G [00:50<00:21, 198MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  57% 5.62G/9.94G [00:50<00:21, 199MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  57% 5.64G/9.94G [00:51<00:21, 200MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  57% 5.67G/9.94G [00:51<00:21, 203MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  57% 5.70G/9.94G [00:51<00:20, 210MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  58% 5.73G/9.94G [00:51<00:20, 208MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  58% 5.76G/9.94G [00:51<00:19, 211MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  58% 5.79G/9.94G [00:51<00:19, 216MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  59% 5.82G/9.94G [00:51<00:18, 221MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  59% 5.85G/9.94G [00:51<00:18, 220MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  59% 5.88G/9.94G [00:52<00:19, 212MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  59% 5.91G/9.94G [00:52<00:19, 202MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  60% 5.93G/9.94G [00:52<00:20, 199MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  60% 5.97G/9.94G [00:52<00:19, 203MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  60% 6.00G/9.94G [00:52<00:18, 210MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  61% 6.02G/9.94G [00:52<00:18, 207MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  61% 6.05G/9.94G [00:52<00:18, 212MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  61% 6.08G/9.94G [00:53<00:17, 216MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  61% 6.11G/9.94G [00:53<00:17, 222MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  62% 6.14G/9.94G [00:53<00:17, 223MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  62% 6.18G/9.94G [00:53<00:17, 212MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  62% 6.21G/9.94G [00:53<00:17, 211MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  63% 6.24G/9.94G [00:53<00:17, 208MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  63% 6.26G/9.94G [00:55<01:06, 55.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  63% 6.28G/9.94G [00:57<02:19, 26.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  63% 6.31G/9.94G [00:57<01:36, 37.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  64% 6.34G/9.94G [00:57<01:08, 52.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  64% 6.38G/9.94G [00:57<00:51, 68.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  64% 6.40G/9.94G [00:57<00:43, 80.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  65% 6.42G/9.94G [00:57<00:37, 94.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  65% 6.44G/9.94G [00:58<00:32, 109MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  65% 6.46G/9.94G [00:58<00:28, 124MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  65% 6.49G/9.94G [00:58<00:23, 148MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  65% 6.51G/9.94G [00:58<00:21, 160MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  66% 6.53G/9.94G [00:58<00:19, 171MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  66% 6.55G/9.94G [00:58<00:19, 178MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  66% 6.59G/9.94G [00:58<00:17, 193MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  67% 6.62G/9.94G [00:58<00:16, 206MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  67% 6.65G/9.94G [00:59<00:15, 208MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  67% 6.68G/9.94G [00:59<00:16, 202MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  67% 6.70G/9.94G [00:59<00:15, 203MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  68% 6.72G/9.94G [00:59<00:15, 203MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  68% 6.75G/9.94G [01:00<00:38, 83.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  68% 6.77G/9.94G [01:00<00:32, 97.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  68% 6.81G/9.94G [01:00<00:26, 120MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  69% 6.83G/9.94G [01:00<00:23, 132MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  69% 6.85G/9.94G [01:00<00:21, 145MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  69% 6.87G/9.94G [01:00<00:19, 156MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  69% 6.89G/9.94G [01:00<00:18, 166MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  69% 6.91G/9.94G [01:00<00:17, 174MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  70% 6.93G/9.94G [01:01<00:16, 181MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  70% 6.95G/9.94G [01:01<00:15, 188MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  70% 6.98G/9.94G [01:01<00:15, 193MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  70% 7.00G/9.94G [01:01<00:15, 194MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  71% 7.03G/9.94G [01:01<00:14, 195MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  71% 7.05G/9.94G [01:01<00:14, 197MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  71% 7.08G/9.94G [01:01<00:13, 208MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  72% 7.11G/9.94G [01:01<00:13, 216MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  72% 7.14G/9.94G [01:02<00:12, 220MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  72% 7.17G/9.94G [01:02<00:13, 198MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  72% 7.20G/9.94G [01:02<00:13, 202MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  73% 7.22G/9.94G [01:02<00:13, 201MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  73% 7.25G/9.94G [01:02<00:13, 202MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  73% 7.28G/9.94G [01:02<00:12, 211MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  74% 7.31G/9.94G [01:02<00:12, 209MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  74% 7.33G/9.94G [01:03<00:12, 206MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  74% 7.36G/9.94G [01:03<00:12, 213MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  74% 7.39G/9.94G [01:03<00:11, 217MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  75% 7.42G/9.94G [01:03<00:11, 221MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  75% 7.46G/9.94G [01:03<00:11, 221MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  75% 7.49G/9.94G [01:03<00:11, 206MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  76% 7.51G/9.94G [01:03<00:12, 203MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  76% 7.53G/9.94G [01:03<00:11, 203MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  76% 7.55G/9.94G [01:04<00:12, 196MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  76% 7.57G/9.94G [01:04<00:12, 197MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  76% 7.59G/9.94G [01:04<00:11, 198MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  77% 7.61G/9.94G [01:04<00:11, 200MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  77% 7.63G/9.94G [01:04<00:11, 201MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  77% 7.67G/9.94G [01:04<00:10, 212MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  77% 7.70G/9.94G [01:04<00:10, 217MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  78% 7.73G/9.94G [01:04<00:10, 219MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  78% 7.76G/9.94G [01:05<00:10, 206MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  78% 7.79G/9.94G [01:05<00:10, 205MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  79% 7.81G/9.94G [01:05<00:10, 204MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  79% 7.83G/9.94G [01:05<00:10, 203MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  79% 7.85G/9.94G [01:05<00:10, 201MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  79% 7.89G/9.94G [01:05<00:10, 205MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  80% 7.91G/9.94G [01:05<00:10, 202MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  80% 7.94G/9.94G [01:05<00:09, 202MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  80% 7.97G/9.94G [01:06<00:09, 209MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  80% 8.00G/9.94G [01:06<00:09, 215MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  81% 8.03G/9.94G [01:06<00:09, 212MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  81% 8.06G/9.94G [01:06<00:08, 212MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  81% 8.10G/9.94G [01:06<00:08, 212MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  82% 8.13G/9.94G [01:06<00:09, 199MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  82% 8.15G/9.94G [01:07<00:09, 184MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  82% 8.17G/9.94G [01:07<00:09, 184MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  82% 8.19G/9.94G [01:07<00:09, 184MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  83% 8.21G/9.94G [01:07<00:09, 187MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  83% 8.23G/9.94G [01:07<00:08, 193MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  83% 8.26G/9.94G [01:07<00:08, 202MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  83% 8.28G/9.94G [01:07<00:08, 188MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  84% 8.30G/9.94G [01:07<00:09, 180MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  84% 8.33G/9.94G [01:07<00:09, 169MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  84% 8.35G/9.94G [01:08<00:09, 169MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  84% 8.37G/9.94G [01:08<00:09, 171MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  84% 8.39G/9.94G [01:08<00:08, 176MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  85% 8.41G/9.94G [01:08<00:12, 119MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  85% 8.43G/9.94G [01:08<00:12, 124MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  85% 8.45G/9.94G [01:08<00:10, 141MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  85% 8.47G/9.94G [01:09<00:10, 144MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  85% 8.49G/9.94G [01:09<00:09, 151MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  86% 8.51G/9.94G [01:09<00:09, 154MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  86% 8.54G/9.94G [01:09<00:08, 160MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  86% 8.56G/9.94G [01:09<00:08, 165MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  86% 8.58G/9.94G [01:11<00:46, 29.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  87% 8.61G/9.94G [01:11<00:29, 44.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  87% 8.64G/9.94G [01:11<00:20, 62.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  87% 8.67G/9.94G [01:12<00:15, 83.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  87% 8.69G/9.94G [01:12<00:13, 95.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  88% 8.71G/9.94G [01:12<00:11, 110MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  88% 8.73G/9.94G [01:12<00:09, 122MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  88% 8.76G/9.94G [01:12<00:08, 138MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  88% 8.79G/9.94G [01:12<00:07, 158MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  89% 8.82G/9.94G [01:12<00:06, 175MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  89% 8.85G/9.94G [01:12<00:05, 187MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  89% 8.87G/9.94G [01:13<00:05, 190MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  90% 8.90G/9.94G [01:13<00:05, 202MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  90% 8.93G/9.94G [01:13<00:04, 212MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  90% 8.97G/9.94G [01:13<00:04, 198MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  90% 9.00G/9.94G [01:13<00:04, 206MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  91% 9.03G/9.94G [01:13<00:04, 213MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  91% 9.06G/9.94G [01:13<00:03, 222MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  91% 9.09G/9.94G [01:14<00:04, 204MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  92% 9.12G/9.94G [01:14<00:04, 201MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  92% 9.14G/9.94G [01:14<00:04, 199MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  92% 9.16G/9.94G [01:14<00:03, 197MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  92% 9.19G/9.94G [01:14<00:03, 199MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  93% 9.21G/9.94G [01:14<00:03, 201MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  93% 9.24G/9.94G [01:14<00:03, 207MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  93% 9.27G/9.94G [01:14<00:03, 209MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  94% 9.30G/9.94G [01:15<00:03, 213MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  94% 9.33G/9.94G [01:15<00:02, 222MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  94% 9.36G/9.94G [01:15<00:02, 225MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  94% 9.40G/9.94G [01:15<00:02, 206MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  95% 9.42G/9.94G [01:15<00:02, 206MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  95% 9.44G/9.94G [01:15<00:02, 204MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  95% 9.46G/9.94G [01:15<00:02, 201MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  95% 9.48G/9.94G [01:15<00:02, 201MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  96% 9.50G/9.94G [01:16<00:02, 200MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  96% 9.52G/9.94G [01:16<00:02, 201MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  96% 9.54G/9.94G [01:16<00:01, 204MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  96% 9.56G/9.94G [01:16<00:01, 199MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  96% 9.59G/9.94G [01:16<00:01, 200MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  97% 9.62G/9.94G [01:16<00:01, 197MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  97% 9.64G/9.94G [01:16<00:01, 199MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  97% 9.66G/9.94G [01:16<00:01, 198MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  97% 9.68G/9.94G [01:16<00:01, 201MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  98% 9.70G/9.94G [01:17<00:01, 200MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  98% 9.72G/9.94G [01:17<00:01, 198MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  98% 9.74G/9.94G [01:17<00:01, 199MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  98% 9.76G/9.94G [01:17<00:00, 199MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  98% 9.78G/9.94G [01:17<00:00, 200MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  99% 9.80G/9.94G [01:17<00:00, 202MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  99% 9.83G/9.94G [01:17<00:00, 198MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  99% 9.86G/9.94G [01:17<00:00, 207MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  99% 9.89G/9.94G [01:17<00:00, 216MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors: 100% 9.92G/9.94G [01:18<00:00, 223MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors: 100% 9.94G/9.94G [01:18<00:00, 127MB/s]\n",
            "Downloading shards:  50% 1/2 [01:19<01:19, 79.16s/it]\n",
            "model-00002-of-00002.safetensors:   0% 0.00/4.54G [00:00<?, ?B/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   0% 21.0M/4.54G [00:00<00:21, 209MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   1% 41.9M/4.54G [00:00<00:23, 193MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   1% 62.9M/4.54G [00:00<00:22, 200MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   2% 83.9M/4.54G [00:00<00:22, 196MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   2% 105M/4.54G [00:03<04:06, 18.0MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   3% 126M/4.54G [00:03<02:49, 26.1MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   3% 157M/4.54G [00:03<01:45, 41.4MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   4% 178M/4.54G [00:03<01:21, 53.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   4% 199M/4.54G [00:03<01:03, 68.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   5% 220M/4.54G [00:04<00:51, 83.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   5% 241M/4.54G [00:04<00:43, 98.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   6% 262M/4.54G [00:04<00:37, 114MB/s] \u001b[A\n",
            "model-00002-of-00002.safetensors:   6% 283M/4.54G [00:04<00:33, 126MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   7% 304M/4.54G [00:04<00:32, 132MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   7% 325M/4.54G [00:04<00:29, 143MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   8% 357M/4.54G [00:04<00:25, 166MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   9% 388M/4.54G [00:04<00:23, 173MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   9% 409M/4.54G [00:05<00:24, 171MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   9% 430M/4.54G [00:05<00:23, 175MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  10% 451M/4.54G [00:05<00:22, 183MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  11% 482M/4.54G [00:05<00:21, 192MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  11% 503M/4.54G [00:07<02:13, 30.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  12% 524M/4.54G [00:07<01:42, 39.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  12% 556M/4.54G [00:08<01:10, 56.3MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  13% 577M/4.54G [00:08<00:57, 69.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  13% 598M/4.54G [00:08<00:46, 84.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  14% 619M/4.54G [00:08<00:39, 98.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  14% 640M/4.54G [00:08<00:35, 109MB/s] \u001b[A\n",
            "model-00002-of-00002.safetensors:  15% 661M/4.54G [00:08<00:31, 122MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  15% 682M/4.54G [00:08<00:28, 137MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  15% 703M/4.54G [00:08<00:25, 151MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  16% 724M/4.54G [00:08<00:23, 163MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  16% 744M/4.54G [00:09<00:21, 173MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  17% 776M/4.54G [00:09<00:20, 187MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  18% 807M/4.54G [00:09<00:18, 199MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  18% 839M/4.54G [00:09<00:17, 206MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  19% 870M/4.54G [00:09<00:17, 212MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  20% 902M/4.54G [00:09<00:16, 217MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  21% 933M/4.54G [00:09<00:16, 218MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  21% 965M/4.54G [00:10<00:17, 205MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  22% 986M/4.54G [00:10<00:17, 206MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  22% 1.01G/4.54G [00:10<00:17, 205MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  23% 1.03G/4.54G [00:10<00:17, 202MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  23% 1.05G/4.54G [00:13<02:40, 21.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  24% 1.08G/4.54G [00:13<01:46, 32.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  24% 1.11G/4.54G [00:13<01:14, 46.1MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  25% 1.14G/4.54G [00:14<00:54, 62.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  26% 1.17G/4.54G [00:14<00:41, 81.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  26% 1.20G/4.54G [00:14<00:35, 94.0MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  27% 1.22G/4.54G [00:14<00:31, 107MB/s] \u001b[A\n",
            "model-00002-of-00002.safetensors:  27% 1.25G/4.54G [00:14<00:25, 130MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  28% 1.27G/4.54G [00:14<00:22, 143MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  28% 1.29G/4.54G [00:14<00:20, 156MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  29% 1.32G/4.54G [00:14<00:18, 177MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  30% 1.35G/4.54G [00:15<00:17, 187MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  30% 1.38G/4.54G [00:15<00:15, 198MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  31% 1.42G/4.54G [00:15<00:15, 199MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  32% 1.45G/4.54G [00:15<00:15, 202MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  33% 1.48G/4.54G [00:15<00:15, 193MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  33% 1.50G/4.54G [00:15<00:17, 178MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  33% 1.52G/4.54G [00:16<00:17, 174MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  34% 1.54G/4.54G [00:16<00:17, 171MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  34% 1.56G/4.54G [00:16<00:17, 169MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  35% 1.58G/4.54G [00:16<00:36, 80.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  35% 1.60G/4.54G [00:16<00:30, 95.1MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  36% 1.63G/4.54G [00:17<00:27, 107MB/s] \u001b[A\n",
            "model-00002-of-00002.safetensors:  36% 1.65G/4.54G [00:17<00:24, 119MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  37% 1.67G/4.54G [00:17<00:21, 131MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  37% 1.69G/4.54G [00:17<00:20, 141MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  38% 1.71G/4.54G [00:17<00:19, 149MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  38% 1.73G/4.54G [00:17<00:17, 158MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  39% 1.75G/4.54G [00:17<00:17, 160MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  39% 1.77G/4.54G [00:17<00:16, 163MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  40% 1.80G/4.54G [00:18<00:15, 180MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  40% 1.82G/4.54G [00:18<00:14, 185MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  41% 1.85G/4.54G [00:18<00:14, 183MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  41% 1.87G/4.54G [00:18<00:15, 174MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  42% 1.89G/4.54G [00:18<00:15, 172MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  42% 1.91G/4.54G [00:18<00:15, 168MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  42% 1.93G/4.54G [00:18<00:15, 165MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  43% 1.95G/4.54G [00:18<00:14, 175MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  43% 1.97G/4.54G [00:19<00:14, 181MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  44% 1.99G/4.54G [00:19<00:14, 181MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  44% 2.01G/4.54G [00:19<00:14, 179MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  45% 2.03G/4.54G [00:19<00:14, 178MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  45% 2.06G/4.54G [00:19<00:13, 180MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  46% 2.08G/4.54G [00:19<00:13, 188MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  46% 2.10G/4.54G [00:21<01:06, 36.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  47% 2.12G/4.54G [00:21<01:08, 35.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  47% 2.14G/4.54G [00:22<00:51, 46.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  48% 2.17G/4.54G [00:22<00:35, 66.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  48% 2.19G/4.54G [00:22<00:29, 81.0MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  49% 2.21G/4.54G [00:22<00:24, 96.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  49% 2.23G/4.54G [00:22<00:20, 114MB/s] \u001b[A\n",
            "model-00002-of-00002.safetensors:  50% 2.25G/4.54G [00:22<00:17, 130MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  50% 2.28G/4.54G [00:22<00:15, 144MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  51% 2.30G/4.54G [00:22<00:14, 157MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  51% 2.32G/4.54G [00:22<00:13, 165MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  51% 2.34G/4.54G [00:23<00:12, 171MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  52% 2.36G/4.54G [00:23<00:12, 180MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  52% 2.38G/4.54G [00:23<00:12, 174MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  53% 2.40G/4.54G [00:23<00:11, 180MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  53% 2.42G/4.54G [00:23<00:11, 187MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  54% 2.44G/4.54G [00:23<00:10, 192MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  54% 2.46G/4.54G [00:23<00:10, 197MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  55% 2.50G/4.54G [00:23<00:09, 210MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  56% 2.53G/4.54G [00:23<00:09, 217MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  56% 2.56G/4.54G [00:24<00:09, 215MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  57% 2.59G/4.54G [00:24<00:09, 207MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  58% 2.61G/4.54G [00:24<00:09, 207MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  58% 2.63G/4.54G [00:24<00:09, 205MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  58% 2.65G/4.54G [00:24<00:09, 206MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  59% 2.67G/4.54G [00:24<00:09, 205MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  60% 2.71G/4.54G [00:24<00:08, 204MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  60% 2.74G/4.54G [00:25<00:08, 210MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  61% 2.76G/4.54G [00:25<00:08, 207MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  61% 2.79G/4.54G [00:25<00:08, 214MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  62% 2.82G/4.54G [00:25<00:07, 218MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  63% 2.85G/4.54G [00:25<00:07, 221MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  64% 2.88G/4.54G [00:25<00:07, 209MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  64% 2.92G/4.54G [00:25<00:07, 207MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  65% 2.95G/4.54G [00:26<00:07, 206MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  65% 2.97G/4.54G [00:27<00:28, 55.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  66% 3.00G/4.54G [00:27<00:20, 73.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  67% 3.03G/4.54G [00:27<00:16, 92.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  67% 3.05G/4.54G [00:27<00:14, 106MB/s] \u001b[A\n",
            "model-00002-of-00002.safetensors:  68% 3.07G/4.54G [00:27<00:12, 119MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  68% 3.09G/4.54G [00:27<00:10, 133MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  69% 3.11G/4.54G [00:28<00:09, 148MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  69% 3.14G/4.54G [00:28<00:08, 158MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  70% 3.16G/4.54G [00:28<00:08, 168MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  70% 3.18G/4.54G [00:28<00:07, 174MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  70% 3.20G/4.54G [00:28<00:07, 182MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  71% 3.22G/4.54G [00:28<00:06, 189MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  71% 3.24G/4.54G [00:28<00:06, 190MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  72% 3.26G/4.54G [00:28<00:06, 194MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  72% 3.28G/4.54G [00:28<00:06, 197MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  73% 3.30G/4.54G [00:28<00:06, 200MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  73% 3.32G/4.54G [00:29<00:06, 199MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  74% 3.34G/4.54G [00:29<00:06, 198MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  74% 3.37G/4.54G [00:29<00:05, 197MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  75% 3.39G/4.54G [00:29<00:05, 196MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  75% 3.41G/4.54G [00:29<00:05, 198MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  76% 3.43G/4.54G [00:29<00:05, 188MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  76% 3.45G/4.54G [00:29<00:05, 186MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  76% 3.47G/4.54G [00:29<00:06, 175MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  77% 3.49G/4.54G [00:30<00:06, 169MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  77% 3.51G/4.54G [00:30<00:05, 174MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  78% 3.53G/4.54G [00:30<00:05, 182MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  79% 3.57G/4.54G [00:30<00:05, 193MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  79% 3.60G/4.54G [00:30<00:04, 202MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  80% 3.62G/4.54G [00:30<00:04, 186MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  80% 3.64G/4.54G [00:30<00:05, 173MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  81% 3.66G/4.54G [00:30<00:05, 165MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  81% 3.68G/4.54G [00:31<00:05, 167MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  82% 3.70G/4.54G [00:31<00:05, 167MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  82% 3.72G/4.54G [00:31<00:04, 176MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  82% 3.74G/4.54G [00:31<00:04, 182MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  83% 3.76G/4.54G [00:31<00:04, 172MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  83% 3.79G/4.54G [00:31<00:04, 169MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  84% 3.81G/4.54G [00:31<00:04, 169MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  84% 3.83G/4.54G [00:31<00:04, 175MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  85% 3.85G/4.54G [00:32<00:03, 175MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  85% 3.87G/4.54G [00:34<00:25, 26.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  86% 3.90G/4.54G [00:34<00:15, 40.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  87% 3.93G/4.54G [00:34<00:10, 57.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  87% 3.96G/4.54G [00:34<00:07, 76.4MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  88% 3.98G/4.54G [00:34<00:06, 89.3MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  88% 4.01G/4.54G [00:35<00:05, 103MB/s] \u001b[A\n",
            "model-00002-of-00002.safetensors:  89% 4.04G/4.54G [00:35<00:04, 125MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  89% 4.06G/4.54G [00:35<00:03, 137MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  90% 4.08G/4.54G [00:35<00:03, 151MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  91% 4.11G/4.54G [00:35<00:02, 166MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  91% 4.13G/4.54G [00:35<00:02, 176MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  92% 4.16G/4.54G [00:35<00:01, 195MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  92% 4.19G/4.54G [00:35<00:01, 206MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  93% 4.23G/4.54G [00:36<00:01, 204MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  94% 4.26G/4.54G [00:36<00:01, 206MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  94% 4.29G/4.54G [00:36<00:01, 202MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  95% 4.31G/4.54G [00:36<00:01, 201MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  95% 4.33G/4.54G [00:36<00:01, 203MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  96% 4.36G/4.54G [00:36<00:00, 209MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  97% 4.39G/4.54G [00:36<00:00, 213MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  97% 4.42G/4.54G [00:37<00:00, 219MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  98% 4.46G/4.54G [00:37<00:00, 221MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  99% 4.49G/4.54G [00:37<00:00, 224MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors: 100% 4.54G/4.54G [00:37<00:00, 121MB/s]\n",
            "Downloading shards: 100% 2/2 [01:56<00:00, 58.42s/it]\n",
            "[INFO|modeling_utils.py:1519] 2024-06-01 10:45:03,765 >> Instantiating MistralForCausalLM model under default dtype torch.float16.\n",
            "[INFO|configuration_utils.py:962] 2024-06-01 10:45:03,767 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 2\n",
            "}\n",
            "\n",
            "Loading checkpoint shards: 100% 2/2 [01:09<00:00, 34.51s/it]\n",
            "[INFO|modeling_utils.py:4280] 2024-06-01 10:46:15,855 >> All model checkpoint weights were used when initializing MistralForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:4288] 2024-06-01 10:46:15,856 >> All the weights of MistralForCausalLM were initialized from the model checkpoint at mistralai/Mistral-7B-Instruct-v0.1.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use MistralForCausalLM for predictions without further training.\n",
            "generation_config.json: 100% 116/116 [00:00<00:00, 806kB/s]\n",
            "[INFO|configuration_utils.py:917] 2024-06-01 10:46:16,047 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.1/snapshots/73068f3702d050a2fd5aa2ca1e612e5036429398/generation_config.json\n",
            "[INFO|configuration_utils.py:962] 2024-06-01 10:46:16,047 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 2\n",
            "}\n",
            "\n",
            "06/01/2024 10:46:16 - INFO - llamafactory.model.utils.checkpointing - Gradient checkpointing enabled.\n",
            "06/01/2024 10:46:16 - INFO - llamafactory.model.utils.attention - Using torch SDPA for faster training and inference.\n",
            "06/01/2024 10:46:16 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.\n",
            "06/01/2024 10:46:16 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA\n",
            "06/01/2024 10:46:16 - INFO - llamafactory.model.utils.misc - Found linear modules: up_proj,o_proj,v_proj,gate_proj,q_proj,k_proj,down_proj\n",
            "06/01/2024 10:46:17 - INFO - llamafactory.model.loader - trainable params: 20971520 || all params: 7262703616 || trainable%: 0.2888\n",
            "[INFO|trainer.py:641] 2024-06-01 10:46:17,382 >> Using auto half precision backend\n",
            "[INFO|trainer.py:2078] 2024-06-01 10:46:17,920 >> ***** Running training *****\n",
            "[INFO|trainer.py:2079] 2024-06-01 10:46:17,920 >>   Num examples = 2,415\n",
            "[INFO|trainer.py:2080] 2024-06-01 10:46:17,921 >>   Num Epochs = 1\n",
            "[INFO|trainer.py:2081] 2024-06-01 10:46:17,921 >>   Instantaneous batch size per device = 16\n",
            "[INFO|trainer.py:2084] 2024-06-01 10:46:17,921 >>   Total train batch size (w. parallel, distributed & accumulation) = 64\n",
            "[INFO|trainer.py:2085] 2024-06-01 10:46:17,922 >>   Gradient Accumulation steps = 4\n",
            "[INFO|trainer.py:2086] 2024-06-01 10:46:17,922 >>   Total optimization steps = 37\n",
            "[INFO|trainer.py:2087] 2024-06-01 10:46:17,929 >>   Number of trainable parameters = 20,971,520\n",
            " 14% 5/37 [01:08<07:17, 13.67s/it]06/01/2024 10:47:26 - INFO - llamafactory.extras.callbacks - {'loss': 0.4643, 'learning_rate': 1.9112e-04, 'epoch': 0.13}\n",
            "{'loss': 0.4643, 'grad_norm': 0.8604745864868164, 'learning_rate': 0.0001911228490388136, 'epoch': 0.13}\n",
            " 27% 10/37 [02:18<06:09, 13.70s/it]06/01/2024 10:48:36 - INFO - llamafactory.extras.callbacks - {'loss': 0.1449, 'learning_rate': 1.6607e-04, 'epoch': 0.26}\n",
            "{'loss': 0.1449, 'grad_norm': 0.746547281742096, 'learning_rate': 0.00016606747233900815, 'epoch': 0.26}\n",
            " 41% 15/37 [03:26<05:03, 13.81s/it]06/01/2024 10:49:44 - INFO - llamafactory.extras.callbacks - {'loss': 0.0840, 'learning_rate': 1.2928e-04, 'epoch': 0.40}\n",
            "{'loss': 0.084, 'grad_norm': 0.5027214884757996, 'learning_rate': 0.00012928227712765504, 'epoch': 0.4}\n",
            " 54% 20/37 [04:34<03:52, 13.68s/it]06/01/2024 10:50:52 - INFO - llamafactory.extras.callbacks - {'loss': 0.0502, 'learning_rate': 8.7298e-05, 'epoch': 0.53}\n",
            "{'loss': 0.0502, 'grad_norm': 0.34759554266929626, 'learning_rate': 8.729821802531212e-05, 'epoch': 0.53}\n",
            " 68% 25/37 [05:41<02:43, 13.59s/it]06/01/2024 10:51:59 - INFO - llamafactory.extras.callbacks - {'loss': 0.0444, 'learning_rate': 4.7569e-05, 'epoch': 0.66}\n",
            "{'loss': 0.0444, 'grad_norm': 0.3968988060951233, 'learning_rate': 4.756927164427685e-05, 'epoch': 0.66}\n",
            " 81% 30/37 [06:51<01:35, 13.71s/it]06/01/2024 10:53:09 - INFO - llamafactory.extras.callbacks - {'loss': 0.0320, 'learning_rate': 1.7149e-05, 'epoch': 0.79}\n",
            "{'loss': 0.032, 'grad_norm': 0.6225523352622986, 'learning_rate': 1.7149035075615794e-05, 'epoch': 0.79}\n",
            " 95% 35/37 [08:00<00:27, 13.79s/it]06/01/2024 10:54:18 - INFO - llamafactory.extras.callbacks - {'loss': 0.0332, 'learning_rate': 1.4384e-06, 'epoch': 0.93}\n",
            "{'loss': 0.0332, 'grad_norm': 0.2568511664867401, 'learning_rate': 1.4384089652291543e-06, 'epoch': 0.93}\n",
            "100% 37/37 [08:26<00:00, 13.59s/it][INFO|trainer.py:2329] 2024-06-01 10:54:44,799 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 506.8699, 'train_samples_per_second': 4.765, 'train_steps_per_second': 0.073, 'train_loss': 0.11650654512482721, 'epoch': 0.98}\n",
            "100% 37/37 [08:26<00:00, 13.70s/it]\n",
            "[INFO|trainer.py:3410] 2024-06-01 10:54:44,801 >> Saving model checkpoint to saves/Mistral-7B-v0.1-Chat/lora/train_2024-06-01-10-40-12\n",
            "[INFO|configuration_utils.py:733] 2024-06-01 10:54:45,251 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.1/snapshots/73068f3702d050a2fd5aa2ca1e612e5036429398/config.json\n",
            "[INFO|configuration_utils.py:796] 2024-06-01 10:54:45,252 >> Model config MistralConfig {\n",
            "  \"architectures\": [\n",
            "    \"MistralForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"model_type\": \"mistral\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"sliding_window\": 4096,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.41.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2513] 2024-06-01 10:54:45,412 >> tokenizer config file saved in saves/Mistral-7B-v0.1-Chat/lora/train_2024-06-01-10-40-12/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2522] 2024-06-01 10:54:45,418 >> Special tokens file saved in saves/Mistral-7B-v0.1-Chat/lora/train_2024-06-01-10-40-12/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =     0.9801\n",
            "  total_flos               =  7156619GF\n",
            "  train_loss               =     0.1165\n",
            "  train_runtime            = 0:08:26.86\n",
            "  train_samples_per_second =      4.765\n",
            "  train_steps_per_second   =      0.073\n",
            "Figure saved at: saves/Mistral-7B-v0.1-Chat/lora/train_2024-06-01-10-40-12/training_loss.png\n",
            "06/01/2024 10:54:45 - WARNING - llamafactory.extras.ploting - No metric eval_loss to plot.\n",
            "[INFO|modelcard.py:450] 2024-06-01 10:54:45,661 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-06-01 10:55:46,456 >> loading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.1/snapshots/73068f3702d050a2fd5aa2ca1e612e5036429398/tokenizer.model\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-06-01 10:55:46,456 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.1/snapshots/73068f3702d050a2fd5aa2ca1e612e5036429398/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-06-01 10:55:46,456 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-06-01 10:55:46,456 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.1/snapshots/73068f3702d050a2fd5aa2ca1e612e5036429398/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-06-01 10:55:46,456 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.1/snapshots/73068f3702d050a2fd5aa2ca1e612e5036429398/tokenizer_config.json\n",
            "06/01/2024 10:55:46 - INFO - llamafactory.data.template - Add pad token: </s>\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "[INFO|configuration_utils.py:733] 2024-06-01 10:55:46,805 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.1/snapshots/73068f3702d050a2fd5aa2ca1e612e5036429398/config.json\n",
            "[INFO|configuration_utils.py:796] 2024-06-01 10:55:46,808 >> Model config MistralConfig {\n",
            "  \"_name_or_path\": \"mistralai/Mistral-7B-Instruct-v0.1\",\n",
            "  \"architectures\": [\n",
            "    \"MistralForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"model_type\": \"mistral\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"sliding_window\": 4096,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.41.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "06/01/2024 10:55:46 - INFO - llamafactory.model.utils.quantization - Quantizing model to 4 bit.\n",
            "06/01/2024 10:55:46 - INFO - llamafactory.model.patcher - Using KV cache for faster generation.\n",
            "[INFO|modeling_utils.py:3474] 2024-06-01 10:55:46,943 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.1/snapshots/73068f3702d050a2fd5aa2ca1e612e5036429398/model.safetensors.index.json\n",
            "[INFO|modeling_utils.py:1519] 2024-06-01 10:55:46,946 >> Instantiating MistralForCausalLM model under default dtype torch.bfloat16.\n",
            "[INFO|configuration_utils.py:962] 2024-06-01 10:55:46,947 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 2\n",
            "}\n",
            "\n",
            "Loading checkpoint shards: 100% 2/2 [01:04<00:00, 32.02s/it]\n",
            "[INFO|modeling_utils.py:4280] 2024-06-01 10:56:53,419 >> All model checkpoint weights were used when initializing MistralForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:4288] 2024-06-01 10:56:53,419 >> All the weights of MistralForCausalLM were initialized from the model checkpoint at mistralai/Mistral-7B-Instruct-v0.1.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use MistralForCausalLM for predictions without further training.\n",
            "[INFO|configuration_utils.py:917] 2024-06-01 10:56:53,513 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.1/snapshots/73068f3702d050a2fd5aa2ca1e612e5036429398/generation_config.json\n",
            "[INFO|configuration_utils.py:962] 2024-06-01 10:56:53,513 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 2\n",
            "}\n",
            "\n",
            "06/01/2024 10:56:53 - INFO - llamafactory.model.utils.attention - Using torch SDPA for faster training and inference.\n",
            "06/01/2024 10:56:53 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.\n",
            "06/01/2024 10:56:53 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA\n",
            "06/01/2024 10:56:54 - INFO - llamafactory.model.adapter - Loaded adapter(s): saves/Mistral-7B-v0.1-Chat/lora/train_2024-06-01-10-40-12\n",
            "06/01/2024 10:56:54 - INFO - llamafactory.model.loader - all params: 7262703616\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 0.0.0.0:7860 <> https://bd22faea76d3f12293.gradio.live\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6mIm8u0rXamk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}