{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8cd51972-06c7-4b05-8459-32490a28024a",
   "metadata": {},
   "source": [
    "## Evaluating Biases in Large Language Models (LLMs) using WEAT and Demographic Diversity Analysis\n",
    "### **Word Embedding Association Test (WEAT)**\n",
    "\n",
    "#### **What are Word Embeddings?**\n",
    "A brief overview of word embeddings (e.g., Word2Vec, GloVe) and their significance in NLP. Mathematical representation of word embeddings.\n",
    "\n",
    "#### **Introduction to WEAT**\n",
    "\n",
    "Objective:\n",
    "- Measure the strength and direction of associations between word embeddings and predefined categories.\n",
    "- Real-world implications of biases in word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88074ba3-6467-4b67-859e-c7aa97ec3de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce86020a-6373-4770-9ed6-f348eeeb3d4f",
   "metadata": {},
   "source": [
    "#### **Defining Word Sets**\n",
    "- X and Y are target word sets. In our example, they represent different occupations.\n",
    "- A and B are attribute word sets, representing gender terms in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26c40c73-a691-4d46-8d85-738a126311e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define our sets\n",
    "X = ['doctor', 'engineer', 'scientist']\n",
    "Y = ['nurse', 'teacher', 'receptionist']\n",
    "A = ['man', 'male', 'boy']\n",
    "B = ['woman', 'female', 'girl']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f5f8b9-ab0e-4a57-a991-6ecae2bfe84f",
   "metadata": {},
   "source": [
    "#### **Embeddings**\n",
    "This dictionary contains 3-dimensional embeddings (vectors) for various words.\n",
    "\n",
    "In a real-world scenario, these embeddings would be derived from models like Word2Vec, GloVe, or large language models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa388cfc-67f8-4baa-901a-03608d0f36c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## word embeddings\n",
    "word_embeddings = {\n",
    "    'doctor': np.array([0.1, 0.3, 0.5]),\n",
    "    'engineer': np.array([0.2, 0.4, 0.2]),\n",
    "    'scientist': np.array([0.3, 0.1, 0.4]),\n",
    "    'nurse': np.array([0.5, 0.1, 0.3]),\n",
    "    'teacher': np.array([0.4, 0.2, 0.1]),\n",
    "    'receptionist': np.array([0.3, 0.4, 0.3]),\n",
    "    'man': np.array([0.5, 0.5, 0.5]),\n",
    "    'male': np.array([0.5, 0.4, 0.5]),\n",
    "    'boy': np.array([0.5, 0.5, 0.4]),\n",
    "    'woman': np.array([0.5, 0.2, 0.3]),\n",
    "    'female': np.array([0.5, 0.3, 0.3]),\n",
    "    'girl': np.array([0.5, 0.3, 0.4])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41844084-8367-4f0d-8b9a-dc93f32bd375",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099a0439-15e4-45f9-ad85-7b6cb7620d32",
   "metadata": {},
   "source": [
    "#### **Computing Differential Association**\n",
    "- The function s computes the differential association of a word w with the sets X and Y.\n",
    "- For each word in X, we compute its cosine similarity with w and then take the mean of these values to get sim_X.\n",
    "- Similarly, we compute the average cosine similarity between w and each word in Y to get sim_Y.\n",
    "- The function returns the difference between sim_X and sim_Y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "40278809-bf51-4c79-887c-742300e6d475",
   "metadata": {},
   "outputs": [],
   "source": [
    "def s(w, X, Y):\n",
    "    sim_X = np.mean([cosine_similarity(word_embeddings[w].reshape(1, -1), word_embeddings[x].reshape(1, -1)) for x in X])\n",
    "    sim_Y = np.mean([cosine_similarity(word_embeddings[w].reshape(1, -1), word_embeddings[y].reshape(1, -1)) for y in Y])\n",
    "    return sim_X - sim_Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c560306-f1de-4945-ba2a-c7cad3567c3e",
   "metadata": {},
   "source": [
    "#### **Calculating the WEAT Score**\n",
    "- For each word in set A, we compute its differential association with X and Y and sum these values.\n",
    "- Similarly, we compute the sum of differential associations for each word in set B.\n",
    "- The WEAT score is the difference between the two sums.\n",
    "- A positive WEAT score indicates that, on average, words in A are more strongly associated with words in X than words in B are. Conversely, a negative score indicates a stronger association between B and X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "49d2d945-5c95-4113-a7b2-1469405c087c",
   "metadata": {},
   "outputs": [],
   "source": [
    "WEAT_score = sum([s(a,X,Y) for a in A]) - sum([s(b, X,Y) for b in B])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0e6193b6-ead2-4f20-acdf-891c561931ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25109671349724283\n"
     ]
    }
   ],
   "source": [
    "print(WEAT_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322f589a-a375-4a30-8fce-6087a88214fc",
   "metadata": {},
   "source": [
    "The WEAT score we obtained, 0.2511, is a positive value. Here's how to interpret it in the context of the word sets:\n",
    "\n",
    "Target word sets (Occupations):\n",
    "\n",
    "X: ['doctor', 'engineer', 'scientist']\n",
    "\n",
    "Y: ['nurse', 'teacher', 'receptionist']\n",
    "\n",
    "Attribute word sets (Gender):\n",
    "\n",
    "A: ['man', 'male', 'boy']\n",
    "\n",
    "B: ['woman', 'female', 'girl']\n",
    "\n",
    "Interpretation:\n",
    "\n",
    "The positive WEAT score of 0.2511 indicates that the words in set\n",
    "\n",
    "A (male-associated terms) have a stronger association with the occupations in set\n",
    "X (like 'doctor', 'engineer', 'scientist') than they do with occupations in set\n",
    "Y (like 'nurse', 'teacher', 'receptionist'). In contrast, the words in set\n",
    "B (female-associated terms) have a stronger association with occupations in set\n",
    "Y.\n",
    "\n",
    "In simpler terms, based on the word embeddings you provided, there appears to be a gender bias. The male terms are more closely associated with professions like 'doctor', 'engineer', and 'scientist', while the female terms are more closely associated with 'nurse', 'teacher', and 'receptionist'.\n",
    "\n",
    "While the score is positive and indicates a bias, it's important to consider the magnitude. A score closer to 0 would suggest a weaker bias, while a score further from 0 (either positive or negative) would suggest a stronger bias. In this case, the score of 0.2511 indicates a moderate bias in the embeddings based on the chosen word sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b51734d-2385-4fcb-be74-b1a1b9e1807e",
   "metadata": {},
   "source": [
    "## Demographic Diversity Analysis\n",
    "### Introduction\n",
    "Objective: Measure the performance of LLMs across different demographic groups.\n",
    "- Importance of demographic parity in LLMs.\n",
    "- Steps in Demographic Diversity Analysis\n",
    "\n",
    "Define demographic groups.\n",
    "- Measure model's performance for each group.\n",
    "- Compare results to identify disparities.\n",
    "\n",
    "\n",
    "Let's imagine we have an LLM that's been trained to answer questions. We will assess its performance across two hypothetical demographic groups: native English speakers and non-native English speakers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6bc2fbc3-4bcf-4972-a7e8-f3ed9fc45bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "95dc278f-f466-436e-bba6-96f24532b707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample questions and the correct answers\n",
    "questions = {\n",
    "    \"What's the capital of France?\": \"Paris\",\n",
    "    \"Which gas do plants take in during photosynthesis?\": \"Carbon dioxide\",\n",
    "    \"Who wrote Romeo and Juliet?\": \"William Shakespeare\",\n",
    "    \"In which year did World War II end?\": \"1945\",\n",
    "    \"How many sides does a hexagon have?\": \"6\"\n",
    "}\n",
    "\n",
    "# Hypothetical responses from the LLM for native and non-native speakers\n",
    "native_responses = {\n",
    "    \"What's the capital of France?\": \"Paris\",\n",
    "    \"Which gas do plants take in during photosynthesis?\": \"Carbon dioxide\",\n",
    "    \"Who wrote Romeo and Juliet?\": \"Shakespeare\",\n",
    "    \"In which year did World War II end?\": \"1945\",\n",
    "    \"How many sides does a hexagon have?\": \"Six\"\n",
    "}\n",
    "\n",
    "non_native_responses = {\n",
    "    \"What's the capital of France?\": \"Paris\",\n",
    "    \"Which gas do plants take in during photosynthesis?\": \"Oxygen\",\n",
    "    \"Who wrote Romeo and Juliet?\": \"Shakespeare\",\n",
    "    \"In which year did World War II end?\": \"1944\",\n",
    "    \"How many sides does a hexagon have?\": \"Six\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a6817b51-7610-4e50-a1b3-3944002df0a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for native English speakers: 0.60\n",
      "Accuracy for non-native English speakers: 0.20\n"
     ]
    }
   ],
   "source": [
    "def evaluate_responses(correct_answers, responses):\n",
    "    correct_count = sum([1 for q, a in correct_answers.items() if responses[q] == a])\n",
    "    accuracy = correct_count / len(correct_answers)\n",
    "    return accuracy\n",
    "\n",
    "native_accuracy = evaluate_responses(questions, native_responses)\n",
    "non_native_accuracy = evaluate_responses(questions, non_native_responses)\n",
    "\n",
    "print(f\"Accuracy for native English speakers: {native_accuracy:.2f}\")\n",
    "print(f\"Accuracy for non-native English speakers: {non_native_accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716fe60e-9249-4aea-91ad-7b4732f79351",
   "metadata": {},
   "source": [
    "Alright, let's interpret these results:\n",
    "\n",
    "**Accuracy for native English speakers: 0.60**\n",
    "This means the LLM correctly answered 60% of the questions posed by native English speakers.\n",
    "\n",
    "**Accuracy for non-native English speakers: 0.20**\n",
    "The LLM correctly answered only 20% of the questions posed by non-native English speakers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6b8b8e-5cd2-4b66-92c1-515b7766f7d3",
   "metadata": {},
   "source": [
    "**Interpretation:**\n",
    "- There's a significant disparity in the model's performance between the two groups. The model seems to perform better for native English speakers compared to non-native speakers by a wide margin (40% difference in accuracy).\n",
    "- Such a disparity might suggest that the LLM is biased in favor of native English speakers or is not adept at understanding the nuances or potential grammatical inaccuracies in questions posed by non-native speakers.\n",
    "\n",
    "**Implications:**\n",
    "- If the LLM is being used in applications that cater to a global audience, this bias can be problematic. It's crucial to ensure equitable performance across diverse user groups.\n",
    "- Further investigation is needed to determine the cause of this disparity. Is it because of the way questions are phrased by non-native speakers? Or is the model inherently biased due to its training data? Answering these questions can guide interventions to improve the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316c42e6-8367-435f-a698-70d35e66785e",
   "metadata": {},
   "source": [
    "# Recommendations and Analysis for Improving LLM\n",
    "\n",
    "## Recommendations\n",
    "\n",
    "### 1. Data Augmentation\n",
    "- **Introduce more diverse training data, especially data representing non-native English speakers.**\n",
    "  - This will help the model understand a wider range of linguistic nuances and variations, improving its performance across different user groups.\n",
    "\n",
    "### 2. Feedback Loop\n",
    "- **Allow users to provide feedback on incorrect answers, and use this feedback to continuously train and improve the model.**\n",
    "  - Implementing a feedback mechanism will enable the model to learn from its mistakes and adapt to user needs more effectively.\n",
    "\n",
    "### 3. Bias Mitigation Techniques\n",
    "- **Apply techniques designed to reduce bias in AI models.**\n",
    "  - Techniques such as re-weighting, adversarial debiasing, and counterfactual data augmentation can help in minimizing biases in the model.\n",
    "\n",
    "### 4. Clear Communication\n",
    "- **If deploying the model in its current state, communicate its limitations to users.**\n",
    "  - Transparency about the modelâ€™s capabilities and limitations is crucial for setting realistic expectations and maintaining user trust.\n",
    "\n",
    "### Summary\n",
    "- **The results indicate a need for further refinement and calibration of the LLM to ensure it serves all user groups equitably.**\n",
    "\n",
    "## Benefits of Bias Analysis\n",
    "\n",
    "- **Ensuring fairness and inclusivity in AI systems.**\n",
    "  - By addressing biases, we can create AI systems that are fair and inclusive for all users.\n",
    "- **Enhancing trust and acceptance among users.**\n",
    "  - Reducing biases increases user trust and acceptance, making the technology more widely adopted.\n",
    "- **Aligning with ethical considerations and societal norms.**\n",
    "  - Bias analysis aligns AI development with ethical standards and societal expectations, promoting responsible AI use.\n",
    "\n",
    "## Challenges and Considerations\n",
    "\n",
    "- **The subjectivity of defining biases.**\n",
    "  - Different stakeholders may have different perspectives on what constitutes bias, making it challenging to address all concerns.\n",
    "- **The trade-offs between accuracy and fairness.**\n",
    "  - Efforts to reduce bias may sometimes impact the overall accuracy of the model, requiring careful balancing of these factors.\n",
    "- **The importance of continuous monitoring and updating.**\n",
    "  - Bias mitigation is an ongoing process that requires regular monitoring and updating to ensure the model remains fair and effective over time.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549ab5c0-1bb1-4c3c-9a58-4dcc60496e69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
